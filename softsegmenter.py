# Copyright 2026 Peter Polák.

# -*- coding: utf-8 -*-

import logging
from statistics import mean
from typing import List, Dict, Any

from sacrebleu.metrics.bleu import BLEU
from argparse import ArgumentParser


import logging
from typing import Any, Dict, List
import unicodedata
from mosestokenizer import MosesTokenizer
import yaml
import json
import os
from multiprocessing import Pool

logger = logging.getLogger(__name__)

INF = float("inf")
PUNCT = set([".", "!", "?", ",", ";", ":", "-", "(", ")"])
CHINESE_PUNCT = set(["。", "！", "？", "，", "；", "：", "—", "（", "）"])
JAPAN_PUNCT = set(["。", "！", "？", "，", "；", "：", "ー", "（", "）"])
# Add Chinese and Japanese punctuation to the set
ALL_PUNCT = PUNCT.union(CHINESE_PUNCT).union(JAPAN_PUNCT)


# Scorers adapted from <https://github.com/facebookresearch/SimulEval>
class Instance:
    """
    Stores the information about the output instances generated by SimulEval.
    """

    def __init__(self, info: Dict[str, Any], latency_unit: str = "word"):
        for key, value in info.items():
            setattr(self, key, value)

        self.reference = info.get("reference", "")
        self.prediction = info.get("prediction", "")
        self.latency_unit = latency_unit
        self.metrics = {}

    @property
    def reference_length(self) -> int:
        return self.string_to_len(self.reference, self.latency_unit)

    @staticmethod
    def string_to_len(string: str, latency_unit: str) -> int:
        if latency_unit == "word":
            return len(string.split(" "))
        elif latency_unit == "char":
            return len(string.strip())
        else:
            assert False, f"Unknown latency unit: {latency_unit}"


class SacreBLEUScorer:
    """
    SacreBLEU Scorer

    Usage:
        :code:`--quality-metrics BLEU`

    Additional command line arguments:

    .. argparse::
        :ref: simuleval.evaluator.scorers.quality_scorer.add_sacrebleu_args
        :passparser:
        :prog:
    """

    def __init__(self, tokenizer: str = "13a") -> None:
        super().__init__()
        self.logger = logging.getLogger("simuleval.scorer.bleu")
        self.tokenizer = tokenizer

    def __call__(self, instances: Dict) -> float:
        try:
            return (
                BLEU(tokenize=self.tokenizer)
                .corpus_score(
                    [ins.prediction for ins in instances.values()],
                    [[ins.reference for ins in instances.values()]],
                )
                .score
            )
        except Exception as e:
            self.logger.error(str(e))
            return 0


class YAALScorer:
    r"""

    Yet Another Average Lagging (YAAL) from
    `Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation`
    (https://arxiv.org/abs/2509.17349)
    """

    def __init__(self, computation_aware: bool = False, is_longform: bool = False):
        self.computation_aware = computation_aware
        self.is_longform = is_longform

    def get_delays_lengths(self, ins: Instance):
        """
        Args:
            ins Instance: one instance

        Returns:
            A tuple with the 3 elements:
            delays (List[Union[float, int]]): Sequence of delays.
            src_len (Union[float, int]): Length of source sequence.
            tgt_len (Union[float, int]): Length of target sequence.
        """
        timestamp_type = "delays" if not self.computation_aware else "elapsed"
        delays = getattr(ins, timestamp_type, None)
        assert delays

        if ins.reference is None:
            tgt_len = len(delays)
        else:
            tgt_len = ins.reference_length
        src_len = ins.source_length
        return delays, src_len, tgt_len

    def compute(self, ins: Instance):
        """
        Function to compute latency on one sentence (instance).

        Args:
            ins: Instance: one instance

        Returns:
            float: the latency score on one sentence.
        """
        delays, source_length, target_length = self.get_delays_lengths(ins)

        # if the instance is longform, we use all the delays
        # otherwise, we only use the delays that are less than source_length
        is_longform = hasattr(ins, "longform") or self.is_longform

        recording_end = (
            ins.recording_end if hasattr(ins, "recording_end") else float("inf")
        )

        if (delays[0] >= source_length and not is_longform) or (
            delays[0] >= recording_end
        ):
            return None
        
        assert source_length > 0, "Source length must be greater than 0."

        YAAL = 0
        gamma = max(len(delays), target_length) / source_length
        tau = 0
        for t_minus_1, d in enumerate(delays):

            if (d >= source_length and not is_longform) or (d >= recording_end):
                break

            YAAL += d - t_minus_1 / gamma
            tau = t_minus_1 + 1

        YAAL /= tau
        return YAAL

    def __call__(self, instances: Dict[int, Instance]) -> float:
        scores = []
        timestamp_type = "elapsed" if self.computation_aware else "delays"
        for index, ins in instances.items():
            delays = getattr(ins, timestamp_type, None)
            if delays is None or len(delays) == 0:
                logger.warn(f"Instance {index} has no delay information. Skipped")
                continue
            score = self.compute(ins)
            if score is None:
                continue
            scores.append(score)

        return mean(scores) if len(scores) > 0 else float("nan")


class AlignmentOperation:
    """Enum for alignment operations."""
    MATCH = 0
    DELETE = 1
    INSERT = 2
    NONE = 3


class Word:
    """
    Represents a word with associated timing information.

    Attributes:
        text (str): The word text.
        delay (float): The delay timestamp.
        seq_id (Optional[int]): Sequence identifier for alignment.
        elapsed (Optional[float]): Elapsed time (for computation-aware delays).
        main (bool): Whether this is a main word (not a subtoken).
        original (Optional[str]): The original word before tokenization.
        recording_length (Optional[float]): Total recording length.
    """
    def __init__(
        self,
        text,
        delay,
        *,
        seq_id=None,
        elapsed=None,
        main=True,
        original_str=None,
        recording_length=None,
    ):
        self.text = text
        self.delay = delay
        self.seq_id = seq_id
        self.elapsed = elapsed
        self.main = main
        self.original = original_str
        self.recording_length = recording_length

    def __repr__(self):
        return f"Word(text={self.text}, delay={self.delay}, elapsed={self.elapsed}, seq_id={self.seq_id}, main={self.main}, original={self.original}, recording_length={self.recording_length})"


def align_sequences(seq1, seq2, metric, char_level):
    """
    Align two sequences maximizing the similarity metric.

    This function implements a dynamic programming algorithm similar to
    Needleman-Wunsch, but with a custom similarity score and no gap penalties.
    It is also similar to Dynamic Time Warping (DTW) but there is no penalty
    for leading/trailing gaps. The algorithm allows for insertions and
    deletions without penalty, but matches are scored based on the Jaccard
    similarity of character sets (or exact match for character-level).
    The alignment is computed to maximize the total similarity score across
    the aligned sequences.

    Args:
        seq1 (List[Word]): First sequence (typically reference).
        seq2 (List[Word]): Second sequence (typically hypothesis).
        metric (callable): Similarity function taking (ref_word, hyp_word, char_level).
        char_level (bool): Whether to use character-level comparison.

    Returns:
        tuple: Two aligned sequences with None for gaps.
    """
    # Initialize the alignment matrix
    n = len(seq1) + 1
    m = len(seq2) + 1
    dp = [[0] * m for _ in range(n)]
    dp_back = [[AlignmentOperation.NONE] * m for _ in range(n)]

    # Fill the first row and column of the matrix
    for i in range(n):
        dp[i][0] = 0
        dp_back[i][0] = AlignmentOperation.DELETE
    for j in range(m):
        dp[0][j] = 0
        dp_back[0][j] = AlignmentOperation.INSERT
    dp[0][0] = 0
    dp_back[0][0] = AlignmentOperation.MATCH
    # Fill the alignment matrix
    for i in range(1, n):
        for j in range(1, m):
            match = dp[i - 1][j - 1] + metric(seq1[i - 1], seq2[j - 1], char_level)
            delete = dp[i - 1][j]
            insert = dp[i][j - 1]
            dp[i][j] = max(match, delete, insert)
            if dp[i][j] == match:
                dp_back[i][j] = AlignmentOperation.MATCH
            elif dp[i][j] == delete:
                dp_back[i][j] = AlignmentOperation.DELETE
            else:
                dp_back[i][j] = AlignmentOperation.INSERT

    # Backtrack to find the alignment
    aligned_seq1 = []
    aligned_seq2 = []
    i, j = n - 1, m - 1
    while i > 0 or j > 0:
        if dp_back[i][j] == AlignmentOperation.MATCH:
            aligned_seq1.append(seq1[i - 1])
            aligned_seq2.append(seq2[j - 1])
            i -= 1
            j -= 1
        elif dp_back[i][j] == AlignmentOperation.DELETE:
            aligned_seq1.append(seq1[i - 1])
            aligned_seq2.append(None)
            i -= 1
        elif dp_back[i][j] == AlignmentOperation.INSERT:
            aligned_seq1.append(None)
            aligned_seq2.append(seq2[j - 1])
            j -= 1
        else:
            break
    aligned_seq1.reverse()
    aligned_seq2.reverse()
    return aligned_seq1, aligned_seq2


def load_reference(reference_segmentation, ref_sentences_file, char_level, offset_delays):
    """
    Prepare the reference sentences for alignment.

    Loads segmentation info from a YAML or JSON file and reference sentences from a text file,
    then converts them into lists of Word objects with sequence IDs.

    Args:
        reference_segmentation (str): Path to the YAML or JSON file with reference speech segmentation.
        ref_sentences_file (str): Path to the reference sentences file.
        char_level (bool): Whether to use character-level units.
        offset_delays (bool): Whether to offset delays relative to the first segment.

    Returns:
        tuple: (words, segmentation, reference_sentences) where words is a list of
            lists of Word objects grouped by recording, segmentation is the parsed
            YAML or JSON data, and reference_sentences is the list of reference strings.
    """
    if reference_segmentation.endswith(".json"):
        with open(reference_segmentation, "r", encoding="utf-8") as file:
            segmentation = json.load(file)
    elif reference_segmentation.endswith(".yaml") or reference_segmentation.endswith(".yml"):
        with open(reference_segmentation, "r", encoding="utf-8") as file:
            segmentation = yaml.load(file, Loader=yaml.CLoader)
    else:
        raise ValueError("Unsupported segmentation file format. Use YAML or JSON.")
    for seg in segmentation:
        seg["duration"] = seg["duration"] * 1000  # Convert to milliseconds
        seg["offset"] = seg["offset"] * 1000  # Convert to milliseconds

    with open(ref_sentences_file, "r", encoding="utf-8") as file:
        reference_sentences = file.readlines()
    reference_sentences = [line.strip() for line in reference_sentences]
    if char_level:
        reference_sentences = [
            sentence.replace(" ", "") for sentence in reference_sentences
        ]

    assert len(segmentation) == len(
        reference_sentences
    ), "Number of segments and reference sentences do not match."

    words = []
    for i, (segment, ref_sentence) in enumerate(zip(segmentation, reference_sentences)):
        if i == 0 or segmentation[i - 1]["wav"] != segment["wav"]:
            first_offset = segment["offset"] if offset_delays else 0
            words.append([])
        if offset_delays:
            segment["offset"] -= first_offset

        ref_sentence = ref_sentence.strip().lower()
        units = list(ref_sentence) if char_level else ref_sentence.split()

        # the delay will be used to ensure that hypothesis words emitted during the previous segment
        # are not aligned to the current segment
        delay = segment["offset"]

        words[-1].extend([Word(unit, delay, seq_id=i) for unit in units])

    return words, segmentation, reference_sentences


def get_segmentation_order(segmentation):
    """
    Extract the unique ordering of audio files from the segmentation.

    Args:
        segmentation (list): Parsed segmentation data from YAML or JSON.

    Returns:
        list: Ordered list of unique audio file names.
    """
    segmentation_order = []
    for segment in segmentation:
        if len(segmentation_order) == 0 or segmentation_order[-1] != segment["wav"]:
            segmentation_order.append(segment["wav"])
    return segmentation_order


def fix_elapsed(words):
    """
    Fix the elapsed time of words.
    SimulEval computes the elatsed time as ELAPSED_i = DELAY_i + TOTAL_RUNTIME_i
    but we need to compute it as NEW_ELAPSED_i = DELAY_i + TOTAL_RUNTIME_i - TOTAL_RUNTIME_{i-1}
    TOTAL_RUNTIME_i = ELAPSED_i - DELAY_i
    NEW_ELAPSED_i = DELAY_i + (ELAPSED_i - DELAY_i) - (ELAPSED_{i-1} - DELAY_{i-1})
    NEW_ELAPSED_i = ELAPSED_i - ELAPSED_{i-1} + DELAY_{i-1}
    """
    new_elapsed = []
    for i, word in enumerate(words):
        if i > 0:
            new_elapsed.append(word.elapsed - words[i - 1].elapsed + words[i - 1].delay)
        else:
            new_elapsed.append(word.elapsed)
    for i, word in enumerate(words):
        word.elapsed = (
            new_elapsed[i] if i == 0 else max(new_elapsed[i], words[i - 1].elapsed)
        )
    return words


def normalize_unicode(text):
    """
    Normalize the unicode text to NFKC form.
    """
    return unicodedata.normalize("NFKC", text)


def load_hypothesis(hypothesis_file, char_level, segmentation_order, fix_elapsed_flag):
    """
    Load the hypothesis sentences for alignment.

    Reads hypothesis outputs from a JSONL file and converts them into lists of
    Word objects with delay and elapsed timing information.

    Args:
        hypothesis_file (str): Path to the hypothesis JSONL file.
        char_level (bool): Whether to use character-level units.
        segmentation_order (list): Ordered list of audio file names for alignment.

    Returns:
        List[List[Word]]: List of lists of Word objects, one per recording.
    """
    hypotheses = {}
    source_lengths = {}
    with open(hypothesis_file, "r", encoding="utf-8") as file:
        for line in file:
            h = json.loads(line.strip())
            h_name = os.path.basename(h["source"][0])
            assert h_name in segmentation_order, f"Missing hypothesis for {h_name}"
            assert h_name not in hypotheses, f"Duplicate hypothesis for {h_name}"
            source_lengths[h_name] = h["source_length"] if "source_length" in h else INF
            # if char_level:
            #     h['prediction'] = h['prediction'].replace(' ', '')
            hypotheses[h_name] = h
    assert len(hypotheses) == len(
        segmentation_order
    ), "Number of hypotheses and segmentation orders do not match."

    hypotheses = [
        hypotheses[segmentation_order[i]] for i in range(len(segmentation_order))
    ]
    source_lengths = [
        source_lengths[segmentation_order[i]] for i in range(len(segmentation_order))
    ]
    assert len(hypotheses) == len(
        source_lengths
    ), "Number of hypotheses and source lengths do not match."
    words = []
    all_have_elapsed = all("elapsed" in h for h in hypotheses)
    for i, (h, l) in enumerate(zip(hypotheses, source_lengths)):
        prediction = normalize_unicode(h["prediction"])
        units = list(prediction) if char_level else prediction.split()
        assert len(units) == len(
            h["delays"]
        ), f"Number of units and delays do not match for hypothesis {i}: {len(units)} vs {len(h['delays'])}"
        if "elapsed" not in h:
            h["elapsed"] = list(h["delays"])
        assert len(units) == len(
            h["elapsed"]
        ), f"Number of units and elapsed times do not match for hypothesis {i}: {len(units)} vs {len(h['elapsed'])}"
        instance_words = []
        for unit, delay, elapsed in zip(units, h["delays"], h["elapsed"]):
            instance_words.append(
                Word(unit, delay, elapsed=elapsed, recording_length=l)
            )
        if fix_elapsed_flag:
            instance_words = fix_elapsed(instance_words)
        words.append(instance_words)
    return words, all_have_elapsed


def process_alignment(ref_words, hyp_words, metric, char_level):
    """
    Process the alignment to assign sequence IDs to hypothesis words.

    After alignment, hypothesis words that were aligned to gaps (None in the
    reference) are re-assigned to the nearest reference segment based on
    similarity scores.

    Args:
        ref_words (List[Optional[Word]]): Aligned reference words (with None for gaps).
        hyp_words (List[Optional[Word]]): Aligned hypothesis words (with None for gaps).
        metric (callable): Similarity function taking (ref_word, hyp_word, char_level).
        char_level (bool): Whether using character-level alignment.

    Returns:
        List[Word]: Processed hypothesis words with assigned sequence IDs.
    """
    assert len(ref_words) == len(
        hyp_words
    ), "Number of reference and hypothesis words do not match."

    def get_next_non_none_ref(i):
        while i < len(ref_words) and ref_words[i] is None:
            i += 1
        if i == len(ref_words):
            return i, None
        return i, ref_words[i]

    new_hyp_words = []
    last_ref = None
    nexti = 0
    for i, (ref, hyp) in enumerate(zip(ref_words, hyp_words)):
        if ref is None and i >= nexti:
            if hyp is not None:
                nexti, next_ref = get_next_non_none_ref(i)
                assert (
                    next_ref is not None or last_ref is not None
                ), "No reference word found."
                next_ref_score = (
                    metric(hyp, next_ref, char_level) if next_ref is not None else -INF
                )
                prev_ref_score = (
                    metric(hyp, last_ref, char_level) if last_ref is not None else -INF
                )
                if next_ref_score > prev_ref_score:
                    ref = next_ref
                else:
                    ref = last_ref
                    nexti = i
                logging.info(
                    f"{hyp.text} aligned to {ref.text if ref else 'none'} with score {next_ref_score:.2f} vs {prev_ref_score:.2f} to {last_ref.text  if last_ref else 'none'}"
                )
        # last_ref can be set to next_ref to avoid non-monotonicity
        if ref is not None and i >= nexti:
            last_ref = ref
        if hyp is not None:
            if ref is None:
                continue
            hyp.seq_id = ref.seq_id
            new_hyp_words.append(hyp)

    return new_hyp_words


def calculate_mwer(ref_words, hyp_words):
    """
    Calculate the Match Word Error Rate (MWER) between reference and hypothesis words.
    """
    assert len(ref_words) == len(
        hyp_words
    ), "Number of reference and hypothesis words do not match."

    mwer = 0
    for ref, hyp in zip(ref_words, hyp_words):
        mwer += ref is not None and hyp is not None
    return mwer / len(ref_words) * 100.0


def metric(ref_word, hyp_word, char_level):
    """
    Compute the similarity metric between two words based on Jaccard similarity of
    character sets (or exact match for character-level). Additionally, if one word
    (or character) is punctuation and the other is not, return a negative score to
    discourage alignment of punctuation with non-punctuation.

    Args:
        ref_word (Word): Reference word.
        hyp_word (Word): Hypothesis word.
        char_level (bool): Whether to use character-level comparison.

    Returns:
        float: Similarity score between the words.
    """
    ref_text = ref_word.text
    hyp_text = hyp_word.text

    # If one text is punctuation and the other is not, return a negative score.
    ref_t = ref_text in PUNCT
    hyp_t = hyp_text in PUNCT
    if ref_t ^ hyp_t:
        return -INF

    # For character-level, compare lowercased texts directly
    if char_level:
        return float(ref_text == hyp_text)

    ref_set = set(ref_text)
    hyp_set = set(hyp_text)
    inter = len(ref_set & hyp_set)
    union = len(ref_set) + len(hyp_set) - inter

    return (inter / union) if union else 0.0


def process_audio(args):
    """
    Process alignment for a single audio recording. Top-level function for multiprocessing.

    Args:
        args: Tuple of (sample_index, ref_words, hyp_words, char_level).

    Returns:
        List[Word]: Processed hypothesis words with assigned sequence IDs.
    """
    i, ref, hyp, char_level = args
    aligned_ref, aligned_hyp = align_sequences(ref, hyp, metric, char_level)
    mwer = calculate_mwer(aligned_ref, aligned_hyp)
    print(f"Number of matched words for recording {i}: {mwer:.1f} %")
    return process_alignment(aligned_ref, aligned_hyp, metric, char_level)


def align_words(ref_words, hyp_words, char_level):
    """
    Align all hypothesis words to reference words across all recordings.

    Runs alignment in parallel using multiprocessing and groups the resulting
    hypothesis words by their assigned reference sequence IDs.

    Args:
        ref_words (List[List[Word]]): Reference words grouped by recording.
        hyp_words (List[List[Word]]): Hypothesis words grouped by recording.
        char_level (bool): Whether to use character-level alignment.

    Returns:
        Dict[int, List[Word]]: Mapping from reference sequence ID to aligned
            hypothesis words.
    """
    assert len(ref_words) == len(
        hyp_words
    ), f"Number of reference and hypothesis audios do not match: {len(ref_words)} vs {len(hyp_words)}"

    new_segmentation = dict()
    # ensure that all reference sequences are aligned
    for inst_ref in ref_words:
        for ref in inst_ref:
            new_segmentation[ref.seq_id] = []

    args_list = [
        (i, ref, hyp, char_level)
        for i, (ref, hyp) in enumerate(zip(ref_words, hyp_words))
    ]
    new_hyp_all = []
    with Pool() as pool:
        results = pool.map(process_audio, args_list)
    assert len(results) == len(
        ref_words
    ), f"Number of results and reference words do not match: {len(results)} vs {len(ref_words)}"
    for result in results:
        new_hyp_all.extend(result)

    # split the new_hyp into segments
    for word in new_hyp_all:
        if word.main:
            new_segmentation[word.seq_id].append(word)
    return new_segmentation


def unicode_normalize(text):
    """Normalize Unicode text to NFKC form."""
    return unicodedata.normalize("NFKC", text)


def tokenize_words(words, lang):
    """
    Tokenize words using Moses tokenizer.

    For Chinese and Japanese (or when lang is None), no tokenization is applied.
    Otherwise, the Moses tokenizer is used to split words into subtokens.

    Args:
        words (List[List[Word]]): List of lists of words grouped by recording.
        lang (Optional[str]): Language code for Moses tokenizer (e.g., 'en', 'de').
            Use None, 'zh', or 'ja' to skip tokenization.

    Returns:
        List[List[Word]]: Tokenized words with subtokens marked (main=False).
    """
    if lang is None or lang == "zh" or lang == "ja":
        # For Chinese and Japanese, we don't need to tokenize
        tokenizer = lambda x: [x]
    else:
        tokenizer = MosesTokenizer(lang=lang, no_escape=True)
    tokenized_words = []
    for recording_words in words:
        tokenized_words.append([])
        for word in recording_words:
            text = unicode_normalize(word.text).lower()
            text = tokenizer(text)
            main = True
            for token in text:
                tokenized_words[-1].append(
                    Word(
                        token,
                        word.delay,
                        elapsed=word.elapsed,
                        seq_id=word.seq_id,
                        main=main,
                        original_str=word.text if main else None,
                        recording_length=word.recording_length,
                    )
                )
                main = False
    return tokenized_words


def evaluate_instances(
    resegmented_instances: List[Instance], tokenizer: str, fix_elapsed_flag: bool, all_have_elapsed: bool
) -> Dict[str, float]:
    """
    Evaluate resegmented instances using YAAL (computation-aware and unaware) and BLEU metrics.

    Args:
        resegmented_instances (List[Instance]): List of resegmented instances.
        tokenizer (str): Tokenizer type for SacreBLEU (e.g., '13a').
        fix_elapsed_flag (bool): Whether to fix elapsed times for computation-aware YAAL.
        all_have_elapsed (bool): Whether all instances have elapsed time information.
    Returns:
        Dict[str, float]: Dictionary with 'ca_unaware_yaal', 'ca_aware_yaal', and 'bleu' scores.
    """
    ca_unaware_yaal_scorer = YAALScorer(is_longform=True)
    ca_aware_yaal_scorer = YAALScorer(computation_aware=True, is_longform=True)
    bleu_scorer = SacreBLEUScorer(tokenizer)
    resegmented_instances_dict = {i: ins for i, ins in enumerate(resegmented_instances)}
    ca_unaware_yaal_score = ca_unaware_yaal_scorer(resegmented_instances_dict)

    if all_have_elapsed:
        ca_aware_yaal_score = ca_aware_yaal_scorer(resegmented_instances_dict)
        if not fix_elapsed_flag:
            if ca_aware_yaal_score - ca_unaware_yaal_score > 2000:
                logger.warning(
                    f"Computation-aware YAAL score {ca_aware_yaal_score} is significantly higher than computation-unaware YAAL score {ca_unaware_yaal_score}. Consider fixing elapsed times with --fix_elapsed flag."
                )
    else:
        ca_aware_yaal_score = float("nan")
        logger.warning("Not all instances have elapsed time information. Computation-aware YAAL score will be set to NaN.")
    bleu_score = bleu_scorer(resegmented_instances_dict)
    return {
        "ca_unaware_yaal": ca_unaware_yaal_score,
        "ca_aware_yaal": ca_aware_yaal_score,
        "bleu": bleu_score,
    }


def resegment(
    reference_segmentation,
    ref_sentences_file,
    hypothesis_file,
    char_level,
    lang,
    output_folder,
    bleu_tokenizer,
    offset_delays,
    fix_elapsed_flag,
):
    """
    Main resegmentation pipeline: load data, align, resegment, and evaluate.

    Applies the SoftSegmenter alignment algorithm from
    `"Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text
    Translation" <https://arxiv.org/abs/2509.17349>`_ to align hypothesis outputs with
    reference segments, then evaluates using YAAL and BLEU metrics.

    Args:
        reference_segmentation (str): Path to the YAML or JSON file with reference speech segmentation.
        ref_sentences_file (str): Path to the reference sentences file.
        hypothesis_file (str): Path to the hypothesis JSONL file.
        char_level (bool): Whether to use character-level units.
        lang (Optional[str]): Language code for Moses tokenizer.
        output_folder (str): Path to the output directory.
        bleu_tokenizer (str): Tokenizer type for SacreBLEU.
        offset_delays (bool): Whether to offset delays relative to the first segment.
        fix_elapsed_flag (bool): Whether to fix elapsed times for computation-aware YAAL.
    """
    # Load reference and hypothesis sentences
    ref_words, segmentation, ref_sentences = load_reference(
        reference_segmentation, ref_sentences_file, char_level, offset_delays
    )
    ref_words = tokenize_words(ref_words, lang)
    segmentation_order = get_segmentation_order(segmentation)
    hyp_words, all_have_elapsed = load_hypothesis(hypothesis_file, char_level, segmentation_order, fix_elapsed_flag)
    hyp_words = tokenize_words(hyp_words, lang)

    # Align words
    new_segmentation = align_words(ref_words, hyp_words, char_level)

    os.makedirs(output_folder, exist_ok=True)

    # Save the new segmentation
    instances = []
    instances_dict = []
    for idx, (seg, ref) in enumerate(zip(segmentation, ref_sentences)):
        new_seg = new_segmentation[idx] if idx in new_segmentation else []
        recording_lengths = [w.recording_length for w in new_seg]
        if len(recording_lengths) == 0:
            recording_length = seg["offset"] + seg["duration"]
        else:
            recording_length = max(recording_lengths)
            assert all(
                [w.recording_length == recording_length for w in new_seg]
            ), f"Recording lengths do not match for segment {idx}: {recording_lengths}"
        prediction = [w.original for w in new_seg if w.original is not None]
        if char_level:
            prediction = "".join(prediction)
        else:
            prediction = " ".join(prediction)
        new_seg_dict = {
            "index": idx,
            "prediction": prediction,
            "reference": ref,
            "source_length": seg["duration"],
            "delays": [w.delay - seg["offset"] for w in new_seg],
            "elapsed": [w.elapsed - seg["offset"] for w in new_seg],
            "recording_end": recording_length - seg["offset"],
        }
        instances_dict.append(new_seg_dict)
        instances.append(
            Instance(new_seg_dict, latency_unit="char" if char_level else "word")
        )

    with open(
        os.path.join(output_folder, "instances.resegmented.json"), "w", encoding="utf-8"
    ) as file:
        file.write(json.dumps(instances_dict, ensure_ascii=False, indent=2) + "\n")

    # Calculate metrics
    scores = evaluate_instances(instances, bleu_tokenizer, fix_elapsed_flag, all_have_elapsed)
    with open(
        os.path.join(output_folder, "scores.resegmented.csv"), "w", encoding="utf-8"
    ) as file:
        file.write("\t".join(scores.keys()) + "\n")
        file.write("\t".join([f"{v:.4f}" for v in scores.values()]) + "\n")


if __name__ == "__main__":

    parser = ArgumentParser(description="Better MWER Segmenter")
    parser.add_argument(
        "--speech_segmentation", type=str, required=True, 
        help=(
            "Path to the YAML or JSON file with speech segmentation. "
            "The file should contain a list of segments with 'wav', 'offset', and 'duration' fields."
        )
    )
    parser.add_argument(
        "--ref_sentences_file",
        type=str,
        required=True,
        help="Path to the reference sentences file.",
    )
    parser.add_argument(
        "--hypothesis_file",
        type=str,
        required=True,
        help="Path to the hypothesis file.",
    )
    parser.add_argument(
        "--char_level", action="store_true", help="Use character level segmentation."
    )
    parser.add_argument(
        "--lang", type=str, default=None, help="Language for Moses tokenizer."
    )
    parser.add_argument(
        "--output_folder",
        type=str,
        required=True,
        help="Output file for new segmentation.",
    )
    parser.add_argument(
        "--bleu_tokenizer", type=str, default="13a", help="Tokenizer for the SacreBLEU scorer."
    )
    parser.add_argument(
        "--offset_delays", action="store_true",
        help=(
            "Offset delays relative to the first segment. "
            "This is useful when the delays in the hypothesis are relative to the start of the first segment, "
            "but we want to align them to the recorging. This is useful when you reconcatenate the segments "
            "and want to ensure that the delays are relative to the recording rather than the start of the "
            "first segment."
        )
    )
    parser.add_argument(
        "--fix_elapsed", action="store_true",
        help=(
            "Fix elapsed times for computation-aware YAAL."
            "SimulEval computes elapsed times as ELAPSED_i = DELAY_i + TOTAL_RUNTIME_i, "
            "but we need to compute it as NEW_ELAPSED_i = DELAY_i + TOTAL_RUNTIME_i - TOTAL_RUNTIME_{i-1} "
            "to ensure that the elapsed time of each word is relative to the previous word rather than the "
            "start of the recording."
        )
    )
    args = parser.parse_args()
    resegment(
        args.speech_segmentation,
        args.ref_sentences_file,
        args.hypothesis_file,
        args.char_level,
        args.lang,
        args.output_folder,
        args.bleu_tokenizer,
        args.offset_delays,
        args.fix_elapsed,
    )
