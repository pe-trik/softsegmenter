# -*- coding: utf-8 -*-

import logging
from statistics import mean
from typing import List, Dict, Any

from sacrebleu.metrics.bleu import BLEU
from argparse import ArgumentParser


import logging
from typing import Any, Dict, List
import unicodedata
from mosestokenizer import MosesTokenizer
import yaml
import json
import os
from multiprocessing import Pool

logger = logging.getLogger(__name__)

INF = float("inf")
PUNCT = set([".", "!", "?", ",", ";", ":", "-", "(", ")"])
CHINESE_PUNCT = set(["。", "！", "？", "，", "；", "：", "—", "（", "）"])
JAPAN_PUNCT = set(["。", "！", "？", "，", "；", "：", "ー", "（", "）"])
# Add Chinese and Japanese punctuation to the set
ALL_PUNCT = PUNCT.union(CHINESE_PUNCT).union(JAPAN_PUNCT)


# Scorers adapted from <https://github.com/facebookresearch/SimulEval>
class Instance:
    """
    Stores the information about the output instances generated by SimulEval.
    """

    def __init__(self, info: Dict[str, Any], latency_unit: str = "word"):
        for key, value in info.items():
            setattr(self, key, value)

        self.reference = info.get("reference", "")
        self.prediction = info.get("prediction", "")
        self.latency_unit = latency_unit
        self.metrics = {}

    @property
    def reference_length(self) -> int:
        return self.string_to_len(self.reference, self.latency_unit)

    @staticmethod
    def string_to_len(string: str, latency_unit: str) -> int:
        if latency_unit == "word":
            return len(string.split(" "))
        elif latency_unit == "char":
            return len(string.strip())
        else:
            assert False, f"Unknown latency unit: {latency_unit}"


class SacreBLEUScorer:
    """
    SacreBLEU Scorer

    Usage:
        :code:`--quality-metrics BLEU`

    Additional command line arguments:

    .. argparse::
        :ref: simuleval.evaluator.scorers.quality_scorer.add_sacrebleu_args
        :passparser:
        :prog:
    """

    def __init__(self, tokenizer: str = "13a") -> None:
        super().__init__()
        self.logger = logging.getLogger("simuleval.scorer.bleu")
        self.tokenizer = tokenizer

    def __call__(self, instances: Dict) -> float:
        try:
            return (
                BLEU(tokenize=self.tokenizer)
                .corpus_score(
                    [ins.prediction for ins in instances.values()],
                    [[ins.reference for ins in instances.values()]],
                )
                .score
            )
        except Exception as e:
            self.logger.error(str(e))
            return 0


class YAALScorer:
    r"""

    Yet Another Average Lagging (YAAL) from
    `Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation`
    (https://arxiv.org/abs/2509.17349)
    """

    def __init__(self, computation_aware: bool = False, is_longform: bool = False):
        self.computation_aware = computation_aware
        self.is_longform = is_longform

    def get_delays_lengths(self, ins: Instance):
        """
        Args:
            ins Instance: one instance

        Returns:
            A tuple with the 3 elements:
            delays (List[Union[float, int]]): Sequence of delays.
            src_len (Union[float, int]): Length of source sequence.
            tgt_len (Union[float, int]): Length of target sequence.
        """
        timestamp_type = "delays" if not self.computation_aware else "elapsed"
        delays = getattr(ins, timestamp_type, None)
        assert delays

        if ins.reference is None:
            tgt_len = len(delays)
        else:
            tgt_len = ins.reference_length
        src_len = ins.source_length
        return delays, src_len, tgt_len

    def compute(self, ins: Instance):
        """
        Function to compute latency on one sentence (instance).

        Args:
            ins: Instance: one instance

        Returns:
            float: the latency score on one sentence.
        """
        delays, source_length, target_length = self.get_delays_lengths(ins)

        # if the instance is longform, we use all the delays
        # otherwise, we only use the delays that are less than source_length
        is_longform = hasattr(ins, "longform") or self.is_longform

        recording_end = (
            ins.recording_end if hasattr(ins, "recording_end") else float("inf")
        )

        if (delays[0] >= source_length and not is_longform) or (
            delays[0] >= recording_end
        ):
            return None

        LAAL = 0
        gamma = max(len(delays), target_length) / source_length
        tau = 0
        for t_minus_1, d in enumerate(delays):

            if (d >= source_length and not is_longform) or (d >= recording_end):
                break

            LAAL += d - t_minus_1 / gamma
            tau = t_minus_1 + 1

        LAAL /= tau
        return LAAL

    def __call__(self, instances: Dict[int, Instance]) -> float:
        scores = []
        timestamp_type = "elapsed" if self.computation_aware else "delays"
        for index, ins in instances.items():
            delays = getattr(ins, timestamp_type, None)
            if delays is None or len(delays) == 0:
                logger.warn(f"Instance {index} has no delay information. Skipped")
                continue
            score = self.compute(ins)
            if score is None:
                continue
            scores.append(score)

        return mean(scores) if len(scores) > 0 else float("nan")


# enum for match deletion and insertion
class Match:
    MATCH = 0
    DELETE = 1
    INSERT = 2
    NONE = 3


class Word:
    def __init__(
        self,
        text,
        delay,
        *,
        seq_id=None,
        elapsed=None,
        main=True,
        original_str=None,
        recording_length=None,
    ):
        self.text = text
        self.delay = delay
        self.seq_id = seq_id
        self.elapsed = elapsed
        self.main = main
        self.original = original_str
        self.recording_length = recording_length

    def __repr__(self):
        return f"Word(text={self.text}, delay={self.delay}, elapsed={self.elapsed}, seq_id={self.seq_id}, main={self.main}, original={self.original}, recording_length={self.recording_length})"


def align_sequences(seq1, seq2, metric, char_level):
    """
    Align two sequences maximizing the given metric.
    """
    # Initialize the alignment matrix
    n = len(seq1) + 1
    m = len(seq2) + 1
    dp = [[0] * m for _ in range(n)]
    dp_back = [[Match.NONE] * m for _ in range(n)]

    # Fill the first row and column of the matrix
    for i in range(n):
        dp[i][0] = 0
        dp_back[i][0] = Match.DELETE
    for j in range(m):
        dp[0][j] = 0
        dp_back[0][j] = Match.INSERT
    dp[0][0] = 0
    dp_back[0][0] = Match.MATCH
    # Fill the alignment matrix
    for i in range(1, n):
        for j in range(1, m):
            match = dp[i - 1][j - 1] + metric(seq1[i - 1], seq2[j - 1], char_level)
            delete = dp[i - 1][j]
            insert = dp[i][j - 1]
            dp[i][j] = max(match, delete, insert)
            if dp[i][j] == match:
                dp_back[i][j] = Match.MATCH
            elif dp[i][j] == delete:
                dp_back[i][j] = Match.DELETE
            else:
                dp_back[i][j] = Match.INSERT

    # Backtrack to find the alignment
    aligned_seq1 = []
    aligned_seq2 = []
    i, j = n - 1, m - 1
    while i > 0 or j > 0:
        if dp_back[i][j] == Match.MATCH:
            aligned_seq1.append(seq1[i - 1])
            aligned_seq2.append(seq2[j - 1])
            i -= 1
            j -= 1
        elif dp_back[i][j] == Match.DELETE:
            aligned_seq1.append(seq1[i - 1])
            aligned_seq2.append(None)
            i -= 1
        elif dp_back[i][j] == Match.INSERT:
            aligned_seq1.append(None)
            aligned_seq2.append(seq2[j - 1])
            j -= 1
        else:
            break
    aligned_seq1.reverse()
    aligned_seq2.reverse()
    return aligned_seq1, aligned_seq2


def load_reference(yaml_file, ref_sentences_file, char_level, offset_delays):
    """
    Prepare the reference sentences for alignment.
    """
    with open(yaml_file, "r", encoding="utf-8") as file:
        segmentation = yaml.load(file, Loader=yaml.CLoader)
        for seg in segmentation:
            seg["duration"] = seg["duration"] * 1000  # Convert to milliseconds
            seg["offset"] = seg["offset"] * 1000  # Convert to milliseconds

    with open(ref_sentences_file, "r", encoding="utf-8") as file:
        reference_sentences = file.readlines()
    reference_sentences = [line.strip() for line in reference_sentences]
    if char_level:
        reference_sentences = [
            sentence.replace(" ", "") for sentence in reference_sentences
        ]

    assert len(segmentation) == len(
        reference_sentences
    ), "Number of segments and reference sentences do not match."

    words = []
    for i, (segment, ref_sentence) in enumerate(zip(segmentation, reference_sentences)):
        if i == 0 or segmentation[i - 1]["wav"] != segment["wav"]:
            first_offset = segment["offset"] if offset_delays else 0
            words.append([])
        if offset_delays:
            segment["offset"] -= first_offset

        ref_sentence = ref_sentence.strip().lower()
        units = list(ref_sentence) if char_level else ref_sentence.split()

        # the delay will be used to ensure that hypothesis words emitted during the previous segment
        # are not aligned to the current segment
        delay = segment["offset"]

        words[-1].extend([Word(unit, delay, seq_id=i) for unit in units])

    return words, segmentation, reference_sentences


def get_segmentation_order(segmentation):
    segmentation_order = []
    for segment in segmentation:
        if len(segmentation_order) == 0 or segmentation_order[-1] != segment["wav"]:
            segmentation_order.append(segment["wav"])
    return segmentation_order


def fix_elapsed(words):
    """
    Fix the elapsed time of words.
    SimulEval computes the elatsed time as ELAPSED_i = DELAY_i + TOTAL_RUNTIME_i
    but we need to compute it as NEW_ELAPSED_i = DELAY_i + TOTAL_RUNTIME_i - TOTAL_RUNTIME_{i-1}
    TOTAL_RUNTIME_i = ELAPSED_i - DELAY_i
    NEW_ELAPSED_i = DELAY_i + (ELAPSED_i - DELAY_i) - (ELAPSED_{i-1} - DELAY_{i-1})
    NEW_ELAPSED_i = ELAPSED_i - ELAPSED_{i-1} + DELAY_{i-1}
    """
    new_elapsed = []
    for i, word in enumerate(words):
        if i > 0:
            new_elapsed.append(word.elapsed - words[i - 1].elapsed + words[i - 1].delay)
        else:
            new_elapsed.append(word.elapsed)
    for i, word in enumerate(words):
        word.elapsed = (
            new_elapsed[i] if i == 0 else max(new_elapsed[i], words[i - 1].elapsed)
        )
    return words


def normalize_unicode(text):
    """
    Normalize the unicode text to NFKC form.
    """
    return unicodedata.normalize("NFKC", text)


def load_hypothesis(hypothesis_file, char_level, segmentation_order):
    """
    Load the hypothesis sentences for alignment.
    """
    hypotheses = {}
    source_lengths = {}
    with open(hypothesis_file, "r", encoding="utf-8") as file:
        for line in file:
            h = json.loads(line.strip())
            h_name = os.path.basename(h["source"][0])
            assert h_name in segmentation_order, f"Missing hypothesis for {h_name}"
            assert h_name not in hypotheses, f"Duplicate hypothesis for {h_name}"
            source_lengths[h_name] = h["source_length"] if "source_length" in h else INF
            # if char_level:
            #     h['prediction'] = h['prediction'].replace(' ', '')
            hypotheses[h_name] = h
    assert len(hypotheses) == len(
        segmentation_order
    ), "Number of hypotheses and segmentation orders do not match."

    hypotheses = [
        hypotheses[segmentation_order[i]] for i in range(len(segmentation_order))
    ]
    source_lengths = [
        source_lengths[segmentation_order[i]] for i in range(len(segmentation_order))
    ]
    assert len(hypotheses) == len(
        source_lengths
    ), "Number of hypotheses and source lengths do not match."

    words = []
    for i, (h, l) in enumerate(zip(hypotheses, source_lengths)):
        prediction = normalize_unicode(h["prediction"])
        units = list(prediction) if char_level else prediction.split()
        assert len(units) == len(
            h["delays"]
        ), f"Number of units and delays do not match for hypothesis {i}: {len(units)} vs {len(h['delays'])}"
        assert len(units) == len(
            h["elapsed"]
        ), f"Number of units and elapsed times do not match for hypothesis {i}: {len(units)} vs {len(h['elapsed'])}"
        instance_words = []
        for unit, delay, elapsed in zip(units, h["delays"], h["elapsed"]):
            instance_words.append(
                Word(unit, delay, elapsed=elapsed, recording_length=l)
            )
        words.append(fix_elapsed(instance_words))
    return words


def process_alignment(ref_words, hyp_words, metric, char_level):
    """
    Process the alignment of reference and hypothesis words.
    """
    assert len(ref_words) == len(
        hyp_words
    ), "Number of reference and hypothesis words do not match."

    def get_next_non_none_ref(i):
        while i < len(ref_words) and ref_words[i] is None:
            i += 1
        if i == len(ref_words):
            return i, None
        return i, ref_words[i]

    new_hyp_words = []
    last_ref = None
    nexti = 0
    for i, (ref, hyp) in enumerate(zip(ref_words, hyp_words)):
        if ref is None and i >= nexti:
            if hyp is not None:
                nexti, next_ref = get_next_non_none_ref(i)
                assert (
                    next_ref is not None or last_ref is not None
                ), "No reference word found."
                next_ref_score = (
                    metric(hyp, next_ref, char_level) if next_ref is not None else -INF
                )
                prev_ref_score = (
                    metric(hyp, last_ref, char_level) if last_ref is not None else -INF
                )
                if next_ref_score > prev_ref_score:
                    ref = next_ref
                else:
                    ref = last_ref
                    nexti = i
                logging.info(
                    f"{hyp.text} aligned to {ref.text if ref else 'none'} with score {next_ref_score:.2f} vs {prev_ref_score:.2f} to {last_ref.text  if last_ref else 'none'}"
                )
        # last_ref can be set to next_ref to avoid non-monotonicity
        if ref is not None and i >= nexti:
            last_ref = ref
        if hyp is not None:
            if ref is None:
                continue
            hyp.seq_id = ref.seq_id
            new_hyp_words.append(hyp)

    return new_hyp_words


def calculate_mwer(ref_words, hyp_words):
    """
    Calculate the Match Word Error Rate (MWER) between reference and hypothesis words.
    """
    assert len(ref_words) == len(
        hyp_words
    ), "Number of reference and hypothesis words do not match."

    mwer = 0
    for ref, hyp in zip(ref_words, hyp_words):
        mwer += ref is not None and hyp is not None
    return mwer / len(ref_words) * 100.0


def metric(ref_word, hyp_word, char_level):

    ref_text = ref_word.text
    hyp_text = hyp_word.text

    # If one text is punctuation and the other is not, return a negative score.
    ref_t = ref_text in PUNCT
    hyp_t = hyp_text in PUNCT
    if ref_t ^ hyp_t:
        return -INF

    # For character-level, compare lowercased texts directly
    if char_level:
        return float(ref_text == hyp_text)

    ref_set = set(ref_text)
    hyp_set = set(hyp_text)
    inter = len(ref_set & hyp_set)
    union = len(ref_set) + len(hyp_set) - inter

    return (inter / union) if union else 0.0


def process_audio(args):
    i, ref, hyp, char_level = args
    aligned_ref, aligned_hyp = align_sequences(ref, hyp, metric, char_level)
    mwer = calculate_mwer(aligned_ref, aligned_hyp)
    print(f"Number of matched words for recording {i}: {mwer:.1f} %")
    return process_alignment(aligned_ref, aligned_hyp, metric, char_level)


def align_words(ref_words, hyp_words, char_level):
    assert len(ref_words) == len(
        hyp_words
    ), f"Number of reference and hypothesis audios do not match: {len(ref_words)} vs {len(hyp_words)}"

    new_segmentation = dict()
    # ensure that all reference sequences are aligned
    for inst_ref in ref_words:
        for ref in inst_ref:
            new_segmentation[ref.seq_id] = []

    args_list = [
        (i, ref, hyp, char_level)
        for i, (ref, hyp) in enumerate(zip(ref_words, hyp_words))
    ]
    new_hyp_all = []
    with Pool() as pool:
        results = pool.map(process_audio, args_list)
    assert len(results) == len(
        ref_words
    ), f"Number of results and reference words do not match: {len(results)} vs {len(ref_words)}"
    for result in results:
        new_hyp_all.extend(result)

    # split the new_hyp into segments
    for word in new_hyp_all:
        if word.main:
            new_segmentation[word.seq_id].append(word)
    return new_segmentation


def unicode_normalize(text):
    return unicodedata.normalize("NFKC", text)


def tokenize_words(words, lang):
    """
    Tokenize words using Moses tokenizer.
    """
    if lang is None or lang == "zh" or lang == "ja":
        # For Chinese and Japanese, we don't need to tokenize
        tokenizer = lambda x: [x]
    else:
        tokenizer = MosesTokenizer(lang=lang, no_escape=True)
    tokenized_words = []
    for recording_words in words:
        tokenized_words.append([])
        for word in recording_words:
            text = unicode_normalize(word.text).lower()
            text = tokenizer(text)
            main = True
            for token in text:
                tokenized_words[-1].append(
                    Word(
                        token,
                        word.delay,
                        elapsed=word.elapsed,
                        seq_id=word.seq_id,
                        main=main,
                        original_str=word.text if main else None,
                        recording_length=word.recording_length,
                    )
                )
                main = False
    return tokenized_words


def evaluate_instances(
    resegmented_instances: List[Instance], tokenizer: str
) -> Dict[str, float]:
    ca_unaware_yaal_scorer = YAALScorer(is_longform=True)
    ca_aware_yaal_scorer = YAALScorer(computation_aware=True, is_longform=True)
    bleu_scorer = SacreBLEUScorer(tokenizer)
    resegmented_instances_dict = {i: ins for i, ins in enumerate(resegmented_instances)}
    ca_unaware_yaal_score = ca_unaware_yaal_scorer(resegmented_instances_dict)
    ca_aware_yaal_score = ca_aware_yaal_scorer(resegmented_instances_dict)
    bleu_score = bleu_scorer(resegmented_instances_dict)
    return {
        "ca_unaware_yaal": ca_unaware_yaal_score,
        "ca_aware_yaal": ca_aware_yaal_score,
        "bleu": bleu_score,
    }


def resegment(
    yaml_file,
    ref_sentences_file,
    hypothesis_file,
    char_level,
    lang,
    output_folder,
    bleu_tokenizer,
    offset_delays,
):
    # Load reference and hypothesis sentences
    ref_words, segmentation, ref_sentences = load_reference(
        yaml_file, ref_sentences_file, char_level, offset_delays
    )
    ref_words = tokenize_words(ref_words, lang)
    segmentation_order = get_segmentation_order(segmentation)
    hyp_words = load_hypothesis(hypothesis_file, char_level, segmentation_order)
    hyp_words = tokenize_words(hyp_words, lang)

    # Align words
    new_segmentation = align_words(ref_words, hyp_words, char_level)

    os.makedirs(output_folder, exist_ok=True)

    # Save the new segmentation
    instances = []
    instances_dict = []
    for idx, (seg, ref) in enumerate(zip(segmentation, ref_sentences)):
        new_seg = new_segmentation[idx] if idx in new_segmentation else []
        recording_lengths = [w.recording_length for w in new_seg]
        if len(recording_lengths) == 0:
            recording_length = seg["offset"] + seg["duration"]
        else:
            recording_length = max(recording_lengths)
            assert all(
                [w.recording_length == recording_length for w in new_seg]
            ), f"Recording lengths do not match for segment {idx}: {recording_lengths}"
        prediction = [w.original for w in new_seg if w.original is not None]
        if char_level:
            prediction = "".join(prediction)
        else:
            prediction = " ".join(prediction)
        new_seg_dict = {
            "index": idx,
            "prediction": prediction,
            "reference": ref,
            "source_length": seg["duration"],
            "delays": [w.delay - seg["offset"] for w in new_seg],
            "elapsed": [w.elapsed - seg["offset"] for w in new_seg],
            "recording_end": recording_length - seg["offset"],
        }
        instances_dict.append(new_seg_dict)
        instances.append(
            Instance(new_seg_dict, latency_unit="char" if char_level else "word")
        )

    with open(
        os.path.join(output_folder, "instances.resegmented.json"), "w", encoding="utf-8"
    ) as file:
        file.write(json.dumps(instances_dict, ensure_ascii=False, indent=2) + "\n")

    # Calculate metrics
    scores = evaluate_instances(instances, bleu_tokenizer)
    with open(
        os.path.join(output_folder, "scores.resegmented.csv"), "w", encoding="utf-8"
    ) as file:
        file.write("\t".join(scores.keys()) + "\n")
        file.write("\t".join([f"{v:.4f}" for v in scores.values()]) + "\n")


if __name__ == "__main__":

    parser = ArgumentParser(description="Better MWER Segmenter")
    parser.add_argument(
        "--yaml_file", type=str, required=True, help="Path to the YAML file."
    )
    parser.add_argument(
        "--ref_sentences_file",
        type=str,
        required=True,
        help="Path to the reference sentences file.",
    )
    parser.add_argument(
        "--hypothesis_file",
        type=str,
        required=True,
        help="Path to the hypothesis file.",
    )
    parser.add_argument(
        "--char_level", action="store_true", help="Use character level segmentation."
    )
    parser.add_argument(
        "--lang", type=str, default=None, help="Language for Moses tokenizer."
    )
    parser.add_argument(
        "--output_folder",
        type=str,
        required=True,
        help="Output file for new segmentation.",
    )
    parser.add_argument(
        "--bleu_tokenizer", type=str, default="13a", help="Tokenizer for BLEU scorer."
    )
    parser.add_argument(
        "--offset_delays", action="store_true", help="Use offset delays."
    )
    args = parser.parse_args()
    resegment(
        args.yaml_file,
        args.ref_sentences_file,
        args.hypothesis_file,
        args.char_level,
        args.lang,
        args.output_folder,
        args.bleu_tokenizer,
        args.offset_delays,
    )
